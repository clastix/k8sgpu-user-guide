apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: inference-service
  template:
    metadata:
      labels:
        app: inference-service
    spec:
      containers:
        - env:
            - name: OLLAMA_NUM_PARALLEL
              value: "4"
            - name: OLLAMA_MAX_LOADED_MODELS
              value: "4"
            - name: OLLAMA_KEEP_ALIVE
              value: "-1"
          image: ollama/ollama:latest
          imagePullPolicy: Always
          name: ollama
          resources:
            requests:
              nvidia.com/gpu: 1
            limits:
              nvidia.com/gpu: 1
        - env:
            - name: OLLAMA_BASE_URL
              value: http://127.0.0.1:11434
          image: ghcr.io/open-webui/open-webui:latest
          imagePullPolicy: Always
          name: open-webui
          resources:
            requests:
              nvidia.com/gpu: 0
            limits:
              nvidia.com/gpu: 0
          ports:
            - containerPort: 8080
      restartPolicy: Always
      runtimeClassName: seeweb-nvidia-1xa100
---
apiVersion: v1
kind: Service
metadata:
  name: inference-service
  labels:
    app: inference-service
spec:
  ports:
  - protocol: TCP
    port: 80 # by convention, the service port is also used by the load-balancer as its public port
    targetPort: 8080
  selector:
    app: inference-service
  type: LoadBalancer
  loadBalancerClass: k8s.gpu # Maake sure to use this loadBalancerClass.
  sessionAffinity: ClientIP
